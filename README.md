# ğŸ“ˆ ESG Report Analysis & Greenwashing Detection Pipeline

This project provides an end-to-end pipeline to automate the analysis of corporate Environmental, Social, and Governance (ESG) reports. It ingests PDF documents, extracts substantive claims, analyzes them across several linguistic dimensions, and calculates a final greenwashing risk score.

---
## ğŸ“‹ Project Overview

The primary goal of this project is to address the challenges of manually analyzing unstructured ESG reports. The pipeline automates the process by:
* **Extracting data** from dense PDF reports.
* **Identifying and isolating** specific corporate claims related to ESG topics.
* **Quantifying each claim** with metrics for specificity, sentiment, and hedging.
* **Producing a structured dataset** that allows for objective analysis and comparison.
* **Calculating an optional greenwashing risk score** to flag claims that may be vague, misleading, or non-committal.

---
## ğŸ“ Folder Structure

To run the project, your files and folders should be organized as follows. The `main.ipynb` notebook should be in the root directory.

project-root/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ Grasim/
â”‚   â”‚   â””â”€â”€ grasim-industries-esg-data-book-2023-2...
â”‚   â”œâ”€â”€ J&J/
â”‚   â”‚   â””â”€â”€ johnson-johnson-2024-health-for-human...
â”‚   â””â”€â”€ sekisui/
â”‚       â””â”€â”€ ESG_factbook_en.pdf
â”‚
â”œâ”€â”€ Approach Document.md
â”œâ”€â”€ main.ipynb
â””â”€â”€ requirements.txt


---
## Environment Setup

Follow these steps in your terminal to set up the necessary environment.

### **1. Create a Virtual Environment (Recommended)**

Isolating your project dependencies in a virtual environment is a best practice.

```bash
# Create a new virtual environment named 'venv'
python -m venv venv

# Activate the virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### **2. Install Python Dependencies**
Install all required Python libraries from the requirements.txt file.

```bash
# Make sure your virtual environment is activated
pip install -r requirements.txt
```

### **3. Download SpaCy Language Model**
The pipeline requires a small English language model from SpaCy for sentence processing.

```bash
#Download the 'en_core_web_sm' model**
python -m spacy download en_core_web_sm
```

## How to Run
Ensure your folder structure and environment are set up as described above.

Open the main.ipynb file in a Jupyter Notebook environment.

Run the cells sequentially from top to bottom to execute the entire pipeline. The final output will be a pandas DataFrame containing the analyzed claims and their risk scores.